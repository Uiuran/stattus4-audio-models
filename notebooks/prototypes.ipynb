{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.variable_scope = tf.compat.v1.variable_scope\n",
    "import numpy as np\n",
    "from cgraphviz import *  \n",
    "from tfhelper import *\n",
    "from ckpt_hookers import *\n",
    "from dataloader import *\n",
    "import argparse\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataload = Sass('/home/penalvad/stattus4/benchfull/', num_samples=10, number_of_batches=10,split_tax=0.8, freq_size=400, time_size=35, same_data_validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataload.sampled_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable tf logging, show DEBUG output\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "'''\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--datadir\", type=str, default=\"/home/penalvad/stattus4/benchfull\",\n",
    "    help=\"data directory\"\n",
    "    )\n",
    "\n",
    "# model\n",
    "parser.add_argument(\n",
    "    \"--model\", type=str, default=\"model.gcnn\",\n",
    "    help=\"model used\"\n",
    "    )\n",
    "\n",
    "# training\n",
    "parser.add_argument(\n",
    "    \"--dataset\", type=str, default=\"data.benchfull\",\n",
    "    help=\"dataset used\"\n",
    "    )\n",
    "\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--epoch\", type=int, default=300,\n",
    "    help=\"training epochs\"\n",
    "    )\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--batchsize\", type=int, default=100,\n",
    "    help=\"batch size\"\n",
    ")\n",
    "\n",
    "# logging / saving\n",
    "parser.add_argument(\n",
    "    \"--logdir\", type=str, default=\"./\",\n",
    "    help=\"log folder\"\n",
    "    )\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--save_mins\", dest=\"save_mins\", default=5, type=int,\n",
    "    help=\"\"\"Save the graph and summaries of once every N steps.\"\"\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--log_steps\", dest=\"log_steps\", default=10, type=int,\n",
    "    help=\"\"\"Log the values of once every N steps.\"\"\"\n",
    ")\n",
    "\n",
    "args = parser.parse_args()\n",
    "tf.summary.text(\"settings\", tf.constant(\" \".join([\"| **\" + i + \"** | `\" + str(k) + \"` |\" for i, k in vars(args).items()\n",
    "'''\n",
    "\n",
    "        # add description text to your log\n",
    "tf.add_to_collection(\"SUMMARIES_ONCE\", tf.summary.text(\"settings\", tf.constant('settings foo'), collections=\"SUMMARIES_ONCE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  with tf.variable_scope('signal_in'):\n",
    "    signal_in = tf.placeholder(tf.float32, shape=(10,40,2,1))\n",
    "\n",
    "  with tf.variable_scope('dascope1'):\n",
    "    conv_linear = tf.keras.layers.Conv2D( 8, (8,2), padding='valid', name='conv_linear', use_bias=True, kernel_initializer=tf.initializers.lecun_normal(seed=137), bias_initializer=tf.initializers.lecun_normal(seed=137) )(signal_in)\n",
    "  \n",
    "  \n",
    "  with tf.variable_scope('softmax'):\n",
    "    logits = tf.contrib.layers.fully_connected(conv_linear, 2, activation_fn=None, normalizer_fn=None, normalizer_params=None, weights_initializer=tf.initializers.lecun_normal(seed=731), weights_regularizer=None, biases_initializer=tf.initializers.lecun_normal(seed=777), biases_regularizer=None, reuse=None, variables_collections=None, outputs_collections=None, trainable=True, scope='logit')\n",
    "    softmax = tf.nn.softmax(logits,axis=0)            \n",
    "    \n",
    "  with tf.variable_scope('loss'):\n",
    "    l_vec = tf.placeholder(tf.float32, shape=(10,2))\n",
    "    globalstep = tf.compat.v1.train.get_or_create_global_step()        \n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0)(l_vec, softmax)         \n",
    "    minimize_op = tf.train.AdamOptimizer(learning_rate=0.05).minimize(loss, global_step = globalstep)    \n",
    "    initop = tf.global_variables_initializer()\n",
    "    graph.add_to_collections(tf.GraphKeys.INIT_OP, initop)\n",
    "    \n",
    "print_display_cgraph(graph)    \n",
    "sess = tf.Session(graph=graph)\n",
    "lvec = [[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1],[1,0],[0,1]]\n",
    "'''\n",
    "with sess.as_default():\n",
    "    \n",
    "  feed_dict = {\n",
    "      signal_in:np.random.rand(10,40,2,1),\n",
    "      l_vec: lvec\n",
    "  }\n",
    "    \n",
    "  sess.run(graph.get_collection_ref(tf.GraphKeys.INIT_OP)[0])\n",
    "  for i in range(2):\n",
    "    lossvalue, _ = sess.run([loss, minimize_op], feed_dict=feed_dict)\n",
    "    print(lossvalue)  \n",
    "'''\n",
    "\n",
    "saver_hook = tf.estimator.CheckpointSaverHook('./ckpt', save_steps=2, checkpoint_basename='experiment.ckpt')\n",
    "with tf.train.MonitoredTrainingSession(hooks=[saver_hook], checkpoint_dir='checkpoint') as sess:\n",
    "\n",
    "  feed_dict = {\n",
    "    signal_in:np.random.rand(10,40,2,1),\n",
    "    l_vec: lvec\n",
    "  }  \n",
    "\n",
    "  while not sess.should_stop():  \n",
    "        \n",
    "    step = sess.run(globalstep)\n",
    "    if (step % 10 == 0):\n",
    "      print(step)\n",
    "      lossvalue, _ = sess.run([loss, minimize_op], feed_dict=feed_dict)\n",
    "      print(lossvalue)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#desambiguatename(opnamescope, scope)\n",
    "#get_listeners(graph, scope='loss/init')\n",
    "op = get_op_fromscope(graph, scope='dascope1/', opname='Conv2D')\n",
    "op = op[0]\n",
    "graphdef = tf.tools.graph_transforms.TransformGraph( graph.as_graph_def(), [], [], ['remove_nodes(op=init)'])\n",
    "\n",
    "with tf.Graph().as_default() as g:  \n",
    "  tf.import_graph_def(graphdef,name = '')\n",
    "  for op in g.get_operations():\n",
    "    if op.name.split('/')[-1] == 'init':\n",
    "      print('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5,
     24,
     68
    ]
   },
   "outputs": [],
   "source": [
    "def copy_all(graph, copy_graph):\n",
    "  for graph_key in graph.collections:\n",
    "    for val in graph.get_collection_ref(graph_key): \n",
    "      copy_graph.add_to_collections(str(graph_key),val)\n",
    "\n",
    "def copy_vars(graph, copy_graph, collection = 'trainable_variables'):\n",
    "    \n",
    "  for tensor in graph.get_collection_ref(collection):\n",
    "    \n",
    "    l = tensor.name.split('/')[:-1]\n",
    "    name = ''\n",
    "    for i in range(len(l)):\n",
    "      if i == 0:\n",
    "        name = name+l[i]\n",
    "      else:\n",
    "        name = name+'/'+l[i]\n",
    "        \n",
    "        \n",
    "    if collection == tf.GraphKeys.TRAIN_OP:    \n",
    "      copy_graph.add_to_collections(tf.GraphKeys.TRAIN_OP, tensor)    \n",
    "    else:\n",
    "      with tf.variable_scope(name, auxiliary_name_scope=False):\n",
    "        tf.get_variable(tensor.name.split(':')[0].split('/')[-1], initializer=copy_graph.get_tensor_by_name(tensor.name), trainable=True) \n",
    "\n",
    "def strip_op(graph, opname, **kwargs):\n",
    "  '''\n",
    "   Returns the same graph without given op\n",
    "  '''\n",
    "  try:\n",
    "    inputops = kwargs['input_ops']\n",
    "  except:\n",
    "    inputops = False\n",
    "    \n",
    "  nodes_after_strip = []\n",
    "  #print(graph.as_graph_def().node)\n",
    "  gd = graph.as_graph_def()\n",
    "  op = graph.get_operation_by_name(opname)\n",
    "  outputs = list(op.outputs)\n",
    "  for node in gd.node:\n",
    "\n",
    "    if node.name.startswith(opname) == False:\n",
    "      copy_node = node_def_pb2.NodeDef()\n",
    "      copy_node.CopyFrom(node)\n",
    "    \n",
    "      # Is the copied node an Output node to the given op ?\n",
    "      for tensorout in outputs:        \n",
    "        outmark = tensorout.name.find(':')   \n",
    "        if copy_node.name == tensorout.name[:outmark]:\n",
    "          new_input = []\n",
    "        \n",
    "          #Setup for the inputs of the copied nodes\n",
    "          for node_name in copy_node.input:\n",
    "      \n",
    "            if node_name.startswith(op.name):\n",
    "              if inputops != False:\n",
    "                new_input.append(inputops)\n",
    "              else:\n",
    "                new_input.append(node_name)\n",
    "        \n",
    "        del copy_node.input[:]\n",
    "        copy_node.input.extend(new_input)        \n",
    "      nodes_after_strip.append(copy_node)   \n",
    "    \n",
    "  output_graphdef = graph_pb2.GraphDef()\n",
    "  output_graphdef.node.extend(nodes_after_strip)\n",
    "  return output_graphdef\n",
    "\n",
    "## FUTURE: strip scope from graph\n",
    "def strip_scope(graph, drop_scope, inputscope = '', ):\n",
    "  '''\n",
    "    Trashy given scope from graph and return the new graph.\n",
    "  '''\n",
    "\n",
    "  #\n",
    "  ##  Output of dropped scope setup\n",
    "  #  Output of another namescope is, by standart transformed into placeholder\n",
    "  #  It returns a dict issuing the new placeholder in a new namescope named after signal_in \n",
    "    \n",
    "  ## Input of dropped scope setup\n",
    "  # Output of the namescope that feeds the dropped scope turnout to be a Variable. \n",
    "   \n",
    "  for node in graph.node:\n",
    "    input_nodes = graph.node\n",
    "    new_node = node_def_pb2.NodeDef()\n",
    "    new_node.CopyFrom(node)\n",
    "    nodes_after_strip = []  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# bagulho experimental doido\n",
    "with tf.Graph().as_default() as graph0:\n",
    "  print_display_cgraph(graph)     \n",
    "  print('Antes Graph')\n",
    "  for i in range(len(graph.collections)):\n",
    "    print(10*'*')\n",
    "    print(graph.collections[i])  \n",
    "    print(graph.get_collection_ref(graph.collections[i]))    \n",
    "  tf.import_graph_def(strip_op(graph,'loss/init'),name='')\n",
    "  \n",
    "  print_display_cgraph(graph0)    \n",
    "  print('Antes Graph0')\n",
    "  for i in range(len(graph0.collections)):\n",
    "    print(10*'*')\n",
    "    print(graph0.collections[i])  \n",
    "    print(graph0.get_collection_ref(graph0.collections[i]))      \n",
    "    \n",
    "  copy_all(graph, graph0)  \n",
    "  graph0.building_function\n",
    "  #copy_vars(graph, graph0, collection=tf.GraphKeys.TRAIN_OP)          \n",
    "  print_display_cgraph(graph0)    \n",
    "  print('Depois Graph0')\n",
    "  for i in range(len(graph0.collections)):\n",
    "    print(10*'*')\n",
    "    print(graph0.collections[i])  \n",
    "    print(graph0.get_collection_ref(graph0.collections[i]))    \n",
    "    \n",
    "  #with tf.variable_scope('loss', auxiliary_name_scope=False):  \n",
    "   # opini = tf.get_variable(graph0.get_collection_ref(tf.GraphKeys.INIT_OP)[0].name)\n",
    "print('Depois Graph')\n",
    "for i in range(len(graph.collections)):\n",
    "  print(10*'*')\n",
    "  print(graph.collections[i])  \n",
    "  print(graph.get_collection_ref(graph.collections[i]))\n",
    "      \n",
    "sess = tf.Session(graph=graph0)\n",
    "\n",
    "with sess.as_default():\n",
    "    \n",
    "  feed_dict = {\n",
    "      graph0.get_tensor_by_name('signal_in/Placeholder:0'):np.random.rand(10,40,2,1),\n",
    "      graph0.get_tensor_by_name('loss/Placeholder:0'): lvec\n",
    "  }\n",
    "  minimize_op = graph0.get_operation_by_name('loss/Adam')\n",
    "  loss = graph0.get_tensor_by_name('loss/categorical_crossentropy/weighted_loss/value:0')  \n",
    "  \n",
    "  sess.run(graph0.get_collection_ref(tf.GraphKeys.INIT_OP))\n",
    "  for i in range(2):\n",
    "    print(i)\n",
    "    lossvalue, _ = sess.run([loss, minimize_op], feed_dict=feed_dict)\n",
    "    #print(lossvalue)   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# oa\n",
    "graph0 = tf.Graph()\n",
    "\n",
    "with graph0.as_default():\n",
    "  with tf.variable_scope('LosDiezMandamientos'):\n",
    "    tf.get_variable('IO', dtype=tf.float32, shape=(1), initializer=None, regularizer=None, trainable=True)\n",
    "    tf.global_variables_initializer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
