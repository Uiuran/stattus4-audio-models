{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Gated-CNN\" data-toc-modified-id=\"Gated-CNN-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Gated CNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#GCNN-1D-Time-Series\" data-toc-modified-id=\"GCNN-1D-Time-Series-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>GCNN 1D Time Series</a></span></li><li><span><a href=\"#GCNN-1D-Residuals\" data-toc-modified-id=\"GCNN-1D-Residuals-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>GCNN 1D Residuals</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#####\n",
    "###  Inicio de refatoracao usando principios de design patterns. \n",
    "# TODO: pesquisar mais e mais e mais a fundo o design do Tensorflow pra seguir de perto.\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class BlockTypes(Enum):\n",
    "  GCNN2D = 'gcnn2d'\n",
    "  SOFTMAX = 'softmax'\n",
    "  CROSSENTROPY = 'loss-crossentropy'\n",
    "  INPUT = 'signal_in'\n",
    "  REDUCEMEAN = 'reducemean'\n",
    "  \n",
    "class ArchBlocks:\n",
    "\n",
    "  def __add__(self, block):\n",
    "    pass\n",
    "    \n",
    "class Architecture(ArchBlocks):\n",
    "  '''\n",
    "    aa\n",
    "  '''\n",
    "  def __init__(self, **kwargs):\n",
    "    \n",
    "    self.modules = {'0':[ArchBlocks.INPUT,0,0,[],[]]}\n",
    "\n",
    "  def __add__(self, archblock1):\n",
    "    pass\n",
    "    #self.modules = archblock1\n",
    "    \n",
    "    \n",
    "  def _block_specification(self,key):    \n",
    "    pass\n",
    "\n",
    "class Stattus4NeuralNetAPI:\n",
    "    \n",
    "  def __init__(self, datatype, datasocket = False, **kwargs):\n",
    "    '''\n",
    "     Input\n",
    "     \n",
    "     datatype: Audio, Pressure, Flow.\n",
    "     datasocket: if True tries to fetch data from a database and push output to database \n",
    "     pre-configured through kwargs, if config is not provided then tries to read from datafeeder or dump to local file-system.   \n",
    "     \n",
    "     Optional\n",
    "     num_of_channels: number of channels for Audio data, if not given get only the first audio and discard the others.    \n",
    "    '''\n",
    "    self.datatype = datatype\n",
    "    self.datasocket = datasocket\n",
    "    \n",
    "  def setup_hyperparameters(self, architectures):\n",
    "    '''\n",
    "      Input\n",
    "      \n",
    "      architectures: Architecture object specifying archetypical structure to be investigated for its hyperparams.\n",
    "\n",
    "      Return a dictionary of the given \n",
    "    '''\n",
    "    pass\n",
    "\n",
    "#######\n",
    "#######\n",
    " ####\n",
    "  ### \n",
    "   #  \n",
    "\n",
    "#  FUTURE: Name/Var Scope handler. This class/template must be used to implement logic of names \n",
    "# according to the architecture block connection. E.G. if a block is just the same from the other in the\n",
    "# sequence them it must be named as if is a deep part of the same architecture blocks. If it is a ramification\n",
    "# of the given below but the same arch them he must be named with another tag.\n",
    "# When duplicating some block to architectonic use, E.G. if you need to frame your data or feature, them each\n",
    "# frame must have an coeherent name space block, like the today ordimatlicly done for repeated name space block,\n",
    "# but this must be automaticly done for identical built blocks of the model, without the need to explicity code it.\n",
    "# Names must be given as unique ID according to te input-output architecture connection, if it has the same IO from previous Block in the sequence,\n",
    "# then each Input-ARchBlock-Output that is IDentical must have the same name with the appropriated tag for deepnes in the sequence of blocks\n",
    "# Automatic update of the name scopes according to the architecture.\n",
    "# Dont change Identical sequential IO Name spaces  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unused building-blocks, will add in the API in the future, after a throughful refatoration.\n",
    "#\n",
    "\n",
    "\n",
    "  self.architectures.gcnn1d = self._gcnn1d\n",
    "  self.architectures.residualgcnn1d = self._residualgcnn1d\n",
    "  def _gcnn1d(self, **kwargs):\n",
    "\n",
    "    try:\n",
    "        \n",
    "      channels_out = kwargs['channels_out']\n",
    "      filter_size = kwargs['filter_size']\n",
    "\n",
    "    except Exception:\n",
    "        \n",
    "      sys.exit('Parameters Not Defined Error')\n",
    "     \n",
    "      signal_in = self(**kwargs)\n",
    "        \n",
    "    # postfix = self.get_namepostfix('gccn1d',**kwargs)\n",
    "      with self.graph.as_default():\n",
    "        with tf.variable_scope('gccn1d'):#+postfix):\n",
    "                             \n",
    "          with self.graph.device(_dev_selector(arg1='foo')):\n",
    "            conv_linear = tf.keras.layers.Conv1D( channels_out, filter_size, padding='causal', name='conv_linear', use_bias=True)(signal_in)\n",
    "        \n",
    "          with self.graph.device(_dev_selector(arg1='foo')):\n",
    "            conv_gate = tf.sigmoid(tf.keras.layers.Conv1D( channels_out, filter_size, padding='causal', name='conv', use_bias=True )(signal_in),name='conv_sigmoid')\n",
    "        \n",
    "          with self.graph.device(_dev_selector(arg1='foo')):\n",
    "            gated_convolutions = tf.multiply(conv_linear,conv_gate,name='gated_convolutions') \n",
    "\n",
    "  def _residualgcnn1d(self, **kwargs):  \n",
    "    \n",
    "    try:\n",
    "        \n",
    "      channels_out = kwargs['channels_out']\n",
    "      filter_size = kwargs['filter_size']\n",
    "        \n",
    "    except KeyError:\n",
    "        \n",
    "      sys.exit('Parameters Not Defined Error')  \n",
    "  \n",
    "    signal_in = self(**kwargs)\n",
    "                               \n",
    "    \n",
    "      ####\n",
    "      ## Keras convolutions. Classes, so dont behave like functions but outputs Tensor, use its functions to query the variables filters and bias\n",
    "                             \n",
    "        # postfix = self.get_namepostfix('gccn1d',**kwargs)\n",
    "    with self.graph.as_default():\n",
    "      with tf.variable_scope('residualgccn1d'):#+postfix):                            \n",
    "                    \n",
    "        with self.graph.device(_dev_selector(arg1='foo')):            \n",
    "          conv_linear = tf.keras.layers.Conv1D( channels_out, filter_size, padding='causal', name='conv_linear', use_bias=True)(signal_in)\n",
    "        with self.graph.device(_dev_selector(arg1='foo')):        \n",
    "          conv_gate = tf.sigmoid(tf.keras.layers.Conv1D( channels_out, filter_size, padding='causal', name='conv', use_bias=True )(signal_in),name='conv_sigmoid')\n",
    "        with self.graph.device(_dev_selector(arg1='foo')):\n",
    "          gated_convolutions = tf.multiply(conv_linear,conv_gate,name='gated_convolutions')\n",
    "        \n",
    "      # Input channels must be the same size of the convolution channels output (ie number of filters applied)\n",
    "      with self.graph.device(_dev_selector(arg1='foo')):\n",
    "        residual = tf.add(gated_convolutions,signal_in,name='residual')\n",
    "        \n",
    "  def _save_trainable_vars(self, blockname):\n",
    "    with self.graph.as_default():   \n",
    "        \n",
    "        \n",
    "      saver = tf.train.Saver(var_list=self.graph.get_collection('trainable_variables') )      \n",
    "      tf.global_variables_initializer()\n",
    "        \n",
    "      sess = tf.Session(graph=self.graph)      \n",
    "      sess.run(self.graph.get_operations()[-1])   \n",
    "      saver.save(sess, os.getcwd()+'/'+blockname)\n",
    "      #saver.export_meta_graph(filename=blockname+'.constructor', collection_list='trainable_variables', export_scope=None, strip_default_attrs=False)\n",
    "      self.arch_blocks[blockname] = blockname\n",
    "        \n",
    "  def define_block(self,blockname):             \n",
    "    \n",
    "    self._save_trainable_vars(blockname)    \n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():                                      \n",
    "      sess = tf.Session(graph=graph)  \n",
    "      \n",
    "      new_saver = tf.train.import_meta_graph(os.getcwd()+'/'+self.arch_blocks[blockname]+'.meta', import_scope = blockname)             \n",
    "      new_saver.restore(sess, os.getcwd()+'/'+self.arch_blocks[blockname])           \n",
    "      ####  \n",
    "      ## Redefinir signal_in dos blocos utilizados para construir este bloco atrav√©s do dict namescopo\n",
    "      ## para que run_cgraph consiga achar inputs  \n",
    "      # \n",
    "        \n",
    "      ####\n",
    "      ## Signalin handling provisorio, fazer um modulo apenas para lidar com signal\n",
    "      #        \n",
    "      self.graph = graph\n",
    "      self.signal_in = []\n",
    "      self.signal_in.append(self.graph.get_tensor_by_name(blockname+'/signal_in:0'))  \n",
    "\n",
    "  # Build block from name in the top level of the blocks, this is a previous version that take names inside namescope dict and build new block  \n",
    "  def from_block(self,archblockname,tag = \"\"):\n",
    "        \n",
    "    with self.graph.as_default():              \n",
    "      sess = tf.Session(graph=self.graph)  \n",
    "\n",
    "      new_saver = tf.train.import_meta_graph(os.getcwd()+'/'+self.arch_blocks[archblockname]+'.meta', import_scope = archblockname+tag)        \n",
    "      new_saver.restore(sess, os.getcwd()+'/'+self.arch_blocks[archblockname])            \n",
    "      \n",
    "      self.signal_in.append(self.graph.get_tensor_by_name(archblockname+tag+'/signal_in:0'))\n",
    "      self.num_input -= 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Gated CNN\n",
    "  Gated CNN is a doubled CNN in whom one of the convoluted signals does the role of opening/closing the network, giving an **Attention Mechanism** to the convolution, for being activated by a sigmoid.\n",
    "  It gives non-vanishing gradient, since the multiplication rule for the derivative applies, also, applies gradient to the linear convoluted part.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCNN 1D Time Series\n",
    "  In time series version, since the desirable learning is based on **past** events, or you cannot uphold the assumption that you have acess to future data, have to make sure that the convolution is **causal**, that is\n",
    "\\begin{align}\n",
    "y_{n}= a_{i}x_{n-i}=a_{n-j}x_{j}\n",
    "\\end{align} \n",
    "  Giving at last, if the filter has length k, k-1 zero padding to the input x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build graph\n",
    "graph = build_graph( (None,l[0],channels), arch = 'gcnn1d',print_ops = True, new_graph=True, show_cgraph = True)\n",
    "'''\n",
    "obs: if you have a graph sometimes is necessary to run more than one time wit reset_default_graph to get a new graph\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCNN 1D Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build graph\n",
    "graph = build_graph( (None,l[0],channels), arch = 'residual gcnn1d',print_ops = True,new_graph=True, show_cgraph = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Param Protocol (hpp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two interfaces implementing the following:\n",
    "\n",
    "A - Space x Selection Algorithm x Architecture\n",
    "\n",
    "- Implementations of this interface specifies the search space for the Deep Learning model, including space size i.e. the number of points and space complexity that is objects to data dict (Variability)/structure/number of sources/Unique Ids (e.g. location). The Space Complex objects should be implement through his own interface.\n",
    "\n",
    "B - Space Complex\n",
    "\n",
    "- Central to space complexity are the specifications of the already given above. Once one have implemented the specification than can implement the Interface itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   HPP Definitions. First DISS  implementation by the already made architecture, brute force selection Algol, \n",
    "# and Space == Space Size.\n",
    "# TODO - refatorar.\n",
    "# TODO - full implementation\n",
    "\n",
    "class Hpp:\n",
    "  '''\n",
    "   Hyperparam tuning protocol.\n",
    "   \n",
    "   Input\n",
    "   - space: Space object giving the size and the complexity of the space\n",
    "   - selection_algorithm: function running algorithm that search through space using specific evaluation (also implemented in the algorithm)\n",
    "   - architecture: Architecture object specifying archetipical architecture to be tuned by searching in space.\n",
    "  '''\n",
    "\n",
    "  def __init__(self,space = {},selection_algorithm = lambda x: 'foo',architecture='gcnn2d'):\n",
    "    \n",
    "    self.space_size = space.get_size()\n",
    "    self.data_structure = space.get_complex().data.get_structure()    \n",
    "    self.source_id = space.get_complex().get_ids()\n",
    "    self.data_dict = space.get_complex().data.get_dict()\n",
    "    self.algorithm = selection_algorithm\n",
    "    self.architecture = architecture\n",
    "    \n",
    "  def _parse_config(config_list):\n",
    "    '''\n",
    "      Receives a config and build from dict to perform  examples:\n",
    "      config0 = [ {\"0_frames\": lambda : 4 1 if 10 == 1 else 0,\"0_0\": lambda :(10,30,40,2), \"0_1\": lambda :(10,2,2,8), \"0_2\": lambda :(10,2,1,16), \"0_3\": lambda :(10,4,1,32), \"0_4\": lambda :(10,8,1,64), \"0_5\": lambda :(10,16,1,128), \"0_6\": lambda :(10,13,1,256)},{\"1_labels\": lambda :2},{\"2_null\": lambda :'a'},{\"3_labels\": lambda :2,\"3_learningrate\": lambda :0.005}]\n",
    "      config1 = [ {\"0_frames\":4,\"0_0\":(10,30,40,2)}, {\"1_1\":(10,2,2,8), \"1_2\":(10,2,1,16), \"1_3\":(10,4,1,32), \"1_4\":(10,8,1,64), \"1_5\":(10,8,1,64)},{\"1_labels\":2},{\"2_null\":a},{\"3_labels\":2,\"3_learningrate\":0.005}]\n",
    "      config2 = [ {\"0_frames\":4,\"0_0\":(10,30,40,2), \"0_1\":(10,2,2,8), \"0_2\":(10,2,1,16), \"0_3\":(10,4,1,32), \"0_4\":(10,8,1,64) },{\"1_labels\":2},{\"2_null\":a},{\"3_labels\":2,\"3_learningrate\":0.005}]      \n",
    "      config3 = [ {\"0_frames\":4,\"0_0\":(10,30,40,2), \"0_1\":(10,2,2,8), \"0_2\":(10,2,1,16), \"0_3\":(10,4,1,32) },{\"1_labels\":2},{\"2_null\":a},{\"3_labels\":2,\"3_learningrate\":0.005}]      \n",
    "      config4 = [ {\"0_frames\":4,\"0_0\":(10,30,40,2), \"0_1\":(10,2,2,8), \"0_2\":(10,2,1,16)},{\"1_labels\":2},{\"2_null\":a},{\"3_labels\":2,\"3_learningrate\":0.005}]      \n",
    "      config5 = [ {\"0_frames\":4,\"0_0\":(10,30,40,2), \"0_1\":(10,2,2,8)},{\"1_labels\":2},{\"2_null\":a},{\"3_labels\":2,\"3_learningrate\":0.005}]      \n",
    "      \n",
    "      config6 = [{\"0_frames\":4,\"0_0\":(10,30,40,2), \"0_1\":(10,2,2,8), \"0_2\":(10,6,1,16), \"0_3\":(10,6,1,32), \"0_4\":(10,6,1,64), \"0_5\":(10,10,1,128), \"0_6\":(10,16,1,256)},{\"1_labels\":2},{\"2_null\":a},{\"3_labels\":2,\"3_learningrate\":0.005}]\n",
    "      config7 = [ {\"0_frames\":4,\"0_0\":(10,30,40,2), \"0_1\":(10,2,2,8), \"0_2\":(10,6,1,16), \"0_3\":(10,6,1,32), \"0_4\":(10,6,1,64), \"0_5\":(10,10,1,128)},{\"1_labels\":2},{\"2_null\":a},{\"3_labels\":2,\"3_learningrate\":0.005}]      \n",
    "      config8 = [ {\"0_frames\":4,\"0_0\":(10,30,40,2), \"0_1\":(10,2,2,8), \"0_2\":(10,6,1,16), \"0_3\":(10,6,1,32), \"0_4\":(10,6,1,64)},{\"1_labels\":2},{\"2_null\":a},{\"3_labels\":2,\"3_learningrate\":0.005}]      \n",
    "      config9 = [ {\"0_frames\":4,\"0_0\":(10,30,40,2), \"0_1\":(10,2,2,8), \"0_2\":(10,6,1,16), \"0_3\":(10,6,1,32)},{\"1_labels\":2},{\"2_null\":a},{\"3_labels\":2,\"3_learningrate\":0.005}]      \n",
    "      config10 = [ {\"0_frames\":4,\"0_0\":(10,30,40,2), \"0_1\":(10,2,2,8), \"0_2\":(10,6,1,16)},{\"1_labels\":2},{\"2_null\":a},{\"3_labels\":2,\"3_learningrate\":0.005}]      \n",
    "      config11 = [ {\"0_frames\":4,\"0_0\":(10,30,40,2), \"0_1\":(10,2,2,8)},{\"1_labels\":2},{\"2_null\":a},{\"3_labels\":2,\"3_learningrate\":0.005}]\n",
    "    '''    \n",
    "    overall_config = []\n",
    "    overall_attr = []\n",
    "    overall_deepness = []\n",
    "    for block_config in config_list:        \n",
    "      layer_configs = []        \n",
    "      layer_attr = []\n",
    "      layer_deep = 0\n",
    "      # attributes       \n",
    "      bc = block_config.items()\n",
    "      for item in bc:\n",
    "        if item[0].split('_')[1].isalnum():\n",
    "          layer_configs.append(item)\n",
    "          layer_deep += 1\n",
    "        else:\n",
    "          layer_attr.append(item)\n",
    "      \n",
    "      layer_configs.sort()\n",
    "      layer_attr.sort()\n",
    "      \n",
    "      overall_deepness.append(layer_deep)\n",
    "      overall_config.append( dict(layer_configs) )\n",
    "      overall_attr.append( dict(layer_attr) )\n",
    "    \n",
    "    return overall_attr,overall_config,overall_deepness \n",
    "    \n",
    "  def _is_vec_embed():\n",
    "    pass\n",
    "\n",
    "  def run(self):    \n",
    "    '''\n",
    "    try: \n",
    "      assert type(tf.get_default_graph()) == type(tf.Graph())\n",
    "      graph = tf.get_default_graph()\n",
    "    except AssertionError:\n",
    "      graph = tf.Graph()         \n",
    "    ''' \n",
    "      \n",
    "    tf.reset_default_graph()\n",
    "    graph = tf.Graph()\n",
    "    s = 0\n",
    "    for config in range(self.algorithm.size):\n",
    "\n",
    "      s += 1\n",
    "      layer_config_list = self.algorithm()\n",
    "      overall_attr,overall_config,overall_deepness = _parse_config(layer_config_list)\n",
    "      \n",
    "      with tf.variable_scope('Config {}'.format(s)):                \n",
    "        \n",
    "        #TODO Assert that it is a block 0 and its attributes (for this case only nframe)            \n",
    "        \n",
    "        blockattr = overall_attr.pop(0)\n",
    "        nframe = blockattr.values()[0]        \n",
    "        config = overall_config.pop(0)     \n",
    "        \n",
    "        for blocks in range(len(self.architecture)):            \n",
    "          \n",
    "          graph = build_graph_module(graph, scope_tensor_name=op.name, arch = self.architecture[blocks], print_ops = True, name_scope = True, show_cgraph = True, filter_size=(k[1],k[2]), channels_out = k[3], deepness = '_d1',num_labels=2,learning_rate=0.005)\n",
    "          op = graph.get_operations()[-1]\n",
    "        \n",
    "          try:\n",
    "            blockattr = overall_attr[0]            \n",
    "            if blockattr.keys()[0].split('_')[0] == blocks:\n",
    "              blockattr = overall_attr.pop(0)\n",
    "              for attr,v in blockattr:\n",
    "                if attr.split('_')[1] == 'frames':\n",
    "                  nframes\n",
    "                \n",
    "          except:\n",
    "            pass                             \n",
    "          \n",
    "        for k in layer_config[2:]:\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            '''\n",
    "        names = []\n",
    "for op in graph.get_operations():\n",
    "    \n",
    "  if op.name.split('/')[1] == 'transpose_1' and op.name.split('/')[0][0:7] == 'softmax':\n",
    "    names.append(op.name)    \n",
    "\n",
    "graph = build_graph_module(graph, scope_tensor_name=names, arch = 'reducemean',print_ops = True, name_scope = True, show_cgraph = True, filter_size=(2,1), channels_out = 64, deepness = '',num_labels=4,learning_rate=0.005, verbose=True)\n",
    "op = graph.get_operations()[-1]\n",
    "graph = build_graph_module(graph, scope_tensor_name=op.name, arch = 'loss-crossentropy',print_ops = True, name_scope = True, show_cgraph = True, filter_size=(2,1), channels_out = 64, deepness = '',num_labels=4,learning_rate=0.1, batch=40, verbose=True)\n",
    "    \n",
    "    layer_config = self.algorithm()\n",
    "    with tf.variable_scope('secondparam'):\n",
    "        \n",
    "      graph = build_graph( (layer_config[0][0],layer_config[0][1],layer_config[0][2],layer_config[0][3]), arch = self.architecture,print_ops = True,new_graph=False, show_cgraph = True, filter_size=(layer_config[1][1],layer_config[1][2]), channels_out = layer_config[1][3])\n",
    "      op = graph.get_operations()[-1]\n",
    "      for k in layer_config[2:]:\n",
    "        graph = build_graph_module(graph, scope_tensor_name=op.name, arch = self.architecture, print_ops = True, name_scope = True, show_cgraph = True, filter_size=(k[1],k[2]), channels_out = k[3], deepness = '_d1',num_labels=2,learning_rate=0.005)\n",
    "        op = graph.get_operations()[-1]\n",
    "\n",
    "      '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Space:\n",
    "  '''\n",
    "   Space is built upon space size and complexity.\n",
    "     \n",
    "   Input:\n",
    "   - size: an integer, the number of configurations to look for the model.      \n",
    "   - spacecomplex: Unique Ids, a hashable type or string specifing source e.g. location. A SigmaComplex object specifying meta-data, i.e. data configurations and \n",
    "   possibly other relevant keys to hyper-param searching such features, diversity mensures and so on.\n",
    "   SigmaComplex must be an interface to be implemented according to the data specification, possibly separating mutable from immutable characters.    \n",
    "  '''    \n",
    "    \n",
    "  def __init__(self, size, spacecomplex):\n",
    "    self.size = size\n",
    "    self.sigmacomplex = spacecomplex\n",
    "        \n",
    "  def get_size(self):\n",
    "    return self.size\n",
    "    \n",
    "  def get_complex(self):\n",
    "    return self.sigmacomplex\n",
    "        \n",
    "  def get_ids(self):\n",
    "    return  self.sigmacomplex.get_ids()\n",
    "    \n",
    "  def get_sourcenum(self):\n",
    "    return self.sigmacomplex.data.sourcenum\n",
    "\n",
    "  def get_dict(self):\n",
    "    return self.sigmacomplex.data.get_dict()\n",
    "\n",
    "  def get_structure(self):\n",
    "    return self.sigmacomplex.data.get_structure()\n",
    "\n",
    "class SigmaComplex:\n",
    "  ''' \n",
    "      Unique Ids, a hashable type or string specifing source e.g. location. A SigmaComplex object specifying meta-data, i.e. data configurations and \n",
    "     possibly other relevant keys to hyper-param searching such features, diversity mensures and so on.\n",
    "     SigmaComplex must be an interface to be implemented according to the data specification, possibly separating mutable from immutable characters.    \n",
    "     \n",
    "     Data specification and ids will correspond to the empirical collected data according to the geo-located point in a device that has known inner workings\n",
    "     according to the laws of physics.\n",
    "  '''\n",
    "  def __init__(self, ids, data):\n",
    "    self.ids = ids\n",
    "    self.data = data   \n",
    "    \n",
    "    \n",
    "  def get_ids(self):\n",
    "    return  self.ids\n",
    "\n",
    "  def get_dict(self):\n",
    "    return self.data.get_dict()\n",
    "\n",
    "  def get_structure(self):\n",
    "    return self.data.get_structure()\n",
    "\n",
    "class DataMeta:\n",
    "  '''\n",
    "   Data MaTter specification according to device collector, its inner workings (laws of physics), and other relevant keys.\n",
    "  '''  \n",
    "  def __init__(self):\n",
    "    \n",
    "    self.structure = \"image\"\n",
    "    self.data_dict = { \"labels\" : [\"cv\",\"sv\"] , \"features\" : [\"spect\",\"framed\"], \"channel_num\": 1 }\n",
    "    \n",
    "  def get_dict(self):\n",
    "    \n",
    "    return self.data_dict\n",
    "    \n",
    "  def get_structure(self):\n",
    "    \n",
    "    return self.structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of 'foo'\n",
    "\n",
    "spacecomplex = SigmaComplex([\"Chala-head-chala\"] , DataMeta())\n",
    "space = Space( 5, spacecomplex)\n",
    "hyperparam = Hpp(space, lambda x: 'È£õËº™Âäü', \"gcnn2d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Build search space data structure. To be added to SigmaComplex class/interface/template\n",
    "#  \"softmax\",\"reducemean\",\"loss-crossentropy\"\n",
    "\n",
    "config0 = [ {\"0_frames\":4,\"0_0\":(10,30,2,2), \"0_1\":(10,2,2,8), \"0_2\":(10,2,1,16), \"0_3\":(10,4,1,32), \"0_4\":(10,8,1,64), \"0_5\":(10,16,1,128), {\"1_labels\":2} ]\n",
    "config1 = [ {\"0_frames\":4,\"0_0\":(10,30,2,2), \"0_1\":(10,2,2,8), \"0_2\":(10,2,1,16), \"0_3\":(10,4,1,32)} ]\n",
    "config2 = [ {\"0_frames\":4,\"0_0\":(10,30,2,2), \"0_1\":(10,2,2,8) ]\n",
    "config3 = [ {\"0_frames\":4,\"0_0\":(10,30,2,2), \"0_1\":(10,4,2,8), \"0_2\":(10,4,1,16), \"0_3\":(10,4,1,32), \"0_4\":(10,4,1,64), \"0_5\":(10,7,1,128), \"0_6\":(10,7,1,256), \"0_7\":(10,7,1,512)}, {\"1_labels\":2},  ]\n",
    "search_space = ( config0,  config1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash( '''( ( {\"0_sld\":4},{\"0_0\":(10,30.0,40.0)}),({\"1_sld\":3},{\"1_0\":(10,30.0,40.0,40.0)} ) )''' )\n",
    "# Example exhaustive search (Ê≤≥ÂçóÁßëÊäÄÂ§ßÂ≠¶)\n",
    "# you can use yield if you want to\n",
    "class search(object):\n",
    "  def __init__(self, listconfig):\n",
    "    self.size = len(listconfig)\n",
    "    self.num = 0\n",
    "    self.config = listconfig\n",
    "   \n",
    "  def __iter__(self):\n",
    "    return self\n",
    "   \n",
    "       # Python 3 compatibility\n",
    "  def __next__(self):\n",
    "    return self.next()\n",
    "   \n",
    "  def next(self):\n",
    "    if self.num < self.size:\n",
    "      cur, self.num = self.config[self.num], self.num+1\n",
    "      return cur\n",
    "    else:\n",
    "      raise StopIteration()\n",
    "  def __call__(self):\n",
    "    return self.next()\n",
    "       \n",
    "spacecomplex = SigmaComplex([\"Chala-head-chala\"] , DataMeta())\n",
    "space = Space( 5, spacecomplex)\n",
    "#TODO implementation of the Space->SigmaComplex->DataMeta()\n",
    "hyperparam = Hpp(space, search( search_space ), [\"gcnn2d\",\"softmax\",\"reducemean\",\"loss-crossentropy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###\n",
    "## Danijar option to define scope with decorators\n",
    "#\n",
    "\n",
    "def doublewrap(function):\n",
    "    \"\"\"\n",
    "    A decorator decorator, allowing to use the decorator to be used without\n",
    "    parentheses if no arguments are provided. All arguments must be optional.\n",
    "    \"\"\"\n",
    "    @functools.wraps(function)\n",
    "    def decorator(*args, **kwargs):\n",
    "        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n",
    "            return function(args[0])\n",
    "        else:\n",
    "            return lambda wrapee: function(wrapee, *args, **kwargs)\n",
    "    return decorator\n",
    "\n",
    "\n",
    "@doublewrap\n",
    "def define_scope(function, scope=None, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    A decorator for functions that define TensorFlow operations. The wrapped\n",
    "    function will only be executed once. Subsequent calls to it will directly\n",
    "    return the result so that operations are added to the graph only once.\n",
    "    The operations added by the function live within a tf.variable_scope(). If\n",
    "    this decorator is used with arguments, they will be forwarded to the\n",
    "    variable scope. The scope name defaults to the name of the wrapped\n",
    "    function.\n",
    "    \"\"\"\n",
    "    attribute = '_cache_' + function.__name__\n",
    "    name = scope or function.__name__\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            with tf.variable_scope(name, *args, **kwargs):\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return decorator\n",
    "#####\n",
    "####\n",
    "##\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_lr_alpha(self, step):   \n",
    "   \n",
    "  self.lalpha = np.abs( (self.lossval[step] + self.lossval[step-1] + self.lossval[step-2] + self.lossval[step-3])/2.0 -self.lossval[step-4] -self.lossval[step-2])\n",
    "  print('l ',self.lalpha)\n",
    "  self.alpha = (self.lossval[step] - self.lossval[step-4])/self.lalpha\n",
    "  print('alpha ', self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        if len(self.logvarloss) >= 2:\n",
    "                \n",
    "            if self.logvarloss[-1] - self.logvarloss[-2] < - 1.0 and self.logvarloss[-1] - self.logvarloss[-2] > - 2.5:\n",
    "                \n",
    "              self.optimizer._lr = self.optimizer._lr + self.optimizer._lr/3.0\n",
    "                \n",
    "            elif self.logvarloss[-1] - self.logvarloss[-2] >  1.0 and self.logvarloss[-1] - self.logvarloss[-2] <  2.5:  \n",
    "                \n",
    "              self.optimizer._lr = self.optimizer._lr - self.optimizer._lr/3.0\n",
    "                \n",
    "            elif self.logvarloss[-1] - self.logvarloss[-2] < - 2.5:\n",
    "                \n",
    "              self.optimizer._lr = self.optimizer._lr + self.optimizer._lr/2.0\n",
    "                \n",
    "            elif self.logvarloss[-1] - self.logvarloss[-2] >  2.5:  \n",
    "                \n",
    "              self.optimizer._lr = self.optimizer._lr - self.optimizer._lr/2.0\n",
    "                \n",
    "            elif self.logvarloss[-1] - self.logvarloss[-2] < 1.0 and self.logvarloss[-1] - self.logvarloss[-2] > - 1.0:  \n",
    "                \n",
    "              self.optimizer._lr = self.optimizer._lr + self.optimizer._lr/2.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
